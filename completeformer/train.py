# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_train.ipynb.

# %% auto 0
__all__ = ['train']

# %% ../nbs/02_train.ipynb 5
import torch
import wandb

import pytorch_lightning as pl

from codecarbon import EmissionsTracker
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar
from pytorch_lightning.loggers import WandbLogger

# %% ../nbs/02_train.ipynb 7
def train(
    model,
    data_module,
    num_epochs,
    output_dir,
    name=None,
    limit_train_batches=1.0,
    limit_val_batches=1.0,
    accumulate_grad_batches=1,
    val_check_interval=20_000,
    ):
    """
    Train a model with a given training data loader, validation data loader,
    optimizer, scheduler, loss function, metrics, and callbacks.

    Args:
        model (LightningModule): The model to train.
        train_dataloader (DataLoader): The training data loader.
        val_dataloader (DataLoader): The validation data loader.
        num_epochs (int): The number of epochs to train for.
        output_dir (str): The directory to save the model to.
        name (str): The name of the model.
    Returns:
        best_model_path (str): The path to the best model's checkpoint.
    """
    pl.seed_everything(115, workers=True)
    wandb_logger = WandbLogger(project="Completeformer", name=name)
    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt
    checkpoint_path = output_dir / "checkpoints"
    checkpoint_callback = ModelCheckpoint(
        monitor="val_loss",
        dirpath=str(checkpoint_path),
        filename="completeformer-{epoch:02d}-{val_loss:.2f}",
        save_top_k=5,
        mode="min",
    )
    trainer = pl.Trainer(
        log_every_n_steps=1,
        logger=wandb_logger,
        default_root_dir=str(checkpoint_path),
        gpus=torch.cuda.device_count(),
        accumulate_grad_batches=accumulate_grad_batches,
        max_epochs=num_epochs,
        limit_train_batches=limit_train_batches,
        limit_val_batches=limit_val_batches,
        precision=16,
        callbacks=[
            checkpoint_callback,
            TQDMProgressBar(refresh_rate=1),
        ],
    )

    # train the model
    trainer.fit(model, data_module)

    # save the last model
    trainer.save_checkpoint(str(checkpoint_path / "final_checkpoint.ckpt"))

    # save the best model to wandb
    best_model_path = checkpoint_callback.best_model_path
    if best_model_path is not None:
        wandb.save(best_model_path)

    return model, checkpoint_callback.best_model_path, trainer
