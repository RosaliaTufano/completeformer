# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_data.ipynb.

# %% auto 0
__all__ = ['ENC_MAX_LEN', 'DEC_MAX_LEN', 'BATCH_SIZE', 'VOCAB_SIZE', 'train_tokenizer', 'tokenize', 'CompleteformerDataset']

# %% ../nbs/00_data.ipynb 5
import random
import torch

import numpy as np
import pytorch_lightning as pl

from dataclasses import dataclass
from datasets import load_dataset
from itertools import chain
from transformers import AutoTokenizer, DataCollatorForSeq2Seq, default_data_collator
from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode
from torch.utils.data import DataLoader
from torch.utils.data._utils.collate import default_collate
from tqdm.auto import tqdm
from typing import Dict, List, Optional

# %% ../nbs/00_data.ipynb 6
ENC_MAX_LEN = 512
DEC_MAX_LEN = 128
BATCH_SIZE = 32
VOCAB_SIZE = 2**15
random.seed(115)

# %% ../nbs/00_data.ipynb 7
def train_tokenizer(train_dataset, vocab_size=2**15):
    # Code modified from: https://huggingface.co/blog/codeparrot

    # Base tokenizer
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    base_vocab = list(bytes_to_unicode().values())

    # Load dataset
    training_corpus = (
        train_dataset[i : i + 1000]["original_method"]
        for i in range(0, len(train_dataset), 1000)
    )

    # Training and saving
    tokenizer = tokenizer.train_new_from_iterator(
        training_corpus,
        vocab_size=vocab_size,
        initial_alphabet=base_vocab
    )
    tokenizer.add_special_tokens(
        {
            "mask_token": "<MASK>",
            "pad_token": "<PAD>",
            "bos_token": "<BOS>",
            "eos_token": "<EOS>",
        }
    )
    tokenizer.add_tokens(["<NEW_LINE>"])
    tokenizer.save_pretrained("completeformer_tokenizer_java")

    return tokenizer

# %% ../nbs/00_data.ipynb 8
def tokenize(examples, tokenizer, enc_max_len, dec_max_len):
    tokenized_example = tokenizer(
        examples["input"],
        padding=False,
        truncation=True,
        max_length=enc_max_len
    )
    examples["target"] = [
        "<BOS>" + x  + "<EOS>" for x in examples["target"]
    ]
    targets = tokenizer(
        examples["target"],
        padding=False,
        truncation=True,
        max_length=dec_max_len
    )
    tokenized_example["labels"] = targets["input_ids"]
    return tokenized_example

# %% ../nbs/00_data.ipynb 9
class CompleteformerDataset(pl.LightningDataModule):
    def __init__(
        self,
        length,
        language,
        tokenizer_name=None,
        batch_size=8,
        enc_max_len=512,
        dec_max_len=128,
        num_workers=4,
        vocab_size=2**15
    ):
        super().__init__()
        self.tokenizer_name = tokenizer_name
        self.batch_size = batch_size
        self.enc_max_len = enc_max_len
        self.dec_max_len = dec_max_len
        self.num_workers = num_workers
        self.vocab_size = vocab_size

        if language == "java":
            self.dataset = load_dataset("semeru/completeformer_java_data", length)
        elif language == "python":
            self.dataset = load_dataset("semeru/completeformer", length)
        else:
            raise ValueError(f"Language {language} not supported. Please choose from java or python.")

        self.train_dataset = self.dataset["train"]
        self.valid_dataset = self.dataset["validation"]
        self.test_dataset = self.dataset["test"]
        
        if self.tokenizer_name is None:
            self.tokenizer = train_tokenizer(self.train_dataset, self.vocab_size)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)
        
        self.data_collator = DataCollatorForSeq2Seq(
            self.tokenizer,
            label_pad_token_id=self.tokenizer.pad_token_id,
            pad_to_multiple_of=8,
        )
    
    def prepare_data(self):
        if "input_ids" not in self.train_dataset.column_names:
            self.train_dataset = self.train_dataset.map(
                lambda x: tokenize(
                    x,
                    self.tokenizer,
                    self.enc_max_len,
                    self.dec_max_len
                ),
                batched=True,
                num_proc=self.num_workers,
                remove_columns=self.train_dataset.column_names,
                load_from_cache_file=False,
            )
            self.valid_dataset = self.valid_dataset.map(
                lambda x: tokenize(
                    x,
                    self.tokenizer,
                    self.enc_max_len,
                    self.dec_max_len
                ),
                batched=True,
                num_proc=self.num_workers,
                remove_columns=self.valid_dataset.column_names,
                load_from_cache_file=False,
            )
            self.test_dataset = self.test_dataset.map(
                lambda x: tokenize(
                    x,
                    self.tokenizer,
                    self.enc_max_len,
                    self.dec_max_len
                ),
                batched=True,
                num_proc=self.num_workers,
                remove_columns=self.test_dataset.column_names,
                load_from_cache_file=False,
            )

            # Set everything to torch tensors
            self.train_dataset.set_format(
                type="torch",
                columns=["input_ids", "attention_mask", "labels"],
            )
            self.valid_dataset.set_format(
                type="torch",
                columns=["input_ids", "attention_mask", "labels"],
            )
            self.test_dataset.set_format(
                type="torch",
                columns=["input_ids", "attention_mask", "labels"],
            )
    
    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            collate_fn=self.data_collator,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.valid_dataset,
            collate_fn=self.data_collator,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            collate_fn=self.data_collator,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers
        )
