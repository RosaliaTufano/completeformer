# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_eval.ipynb.

# %% auto 0
__all__ = ['PY_LANGUAGE', 'parser', 'merge_completions', 'FunctionalEvaluator', 'first_block', 'complete_code',
           'get_block_offsets', 'HumanEvalEvaluator']

# %% ../nbs/03_eval.ipynb 5
import ast
import completeformer
import json
import os
import re

import numpy as np

from datasets import load_dataset, load_metric
from pathlib import Path
from tree_sitter import Language, Parser
from tqdm.auto import tqdm

# %% ../nbs/03_eval.ipynb 7
def merge_completions(code, pred_tokens, tokenizer, mask_token="<mask>"):
    # Generate all valid completions by greedily merging every subset of tokens
    
    completions = []
    for i in range(len(pred_tokens)):
        pred = tokenizer.decode(pred_tokens[:i + 1], skip_special_tokens=True)
        completion = code.replace(mask_token, pred)
        # from: https://stackoverflow.com/a/35796569
        try:
            ast.parse(completion)
            completions.append(completion)
        except SyntaxError as e:
            continue
    
    return completions

# %% ../nbs/03_eval.ipynb 8
class FunctionalEvaluator:
    """
        Evaluate the functional correctness of the model on the given dataset.
    """
    def __init__(self, dataset):
        self.dataset = dataset

    def evaluate(self, model, tokenizer, num_tokens=32):
        """
            Evaluate the model on the given dataset.

            Args:
                model: The model to evaluate.
                tokenizer: The tokenizer used to tokenize the code.
                num_tokens: The number of tokens to generate.
            Returns:
                results (dict): A dictionary containing the results.
        """
        def assert_functionality(code, assertion):
            # Assert that the code is correct by executing it.
            program = code + "\n" + assertion
            try:
                exec(program, locals(), locals())
                return 1
            except Exception as e:
                # TODO: add logging the type of error the model produced
                # print(program)
                # print(e)
                return 0
        
        model.eval()

        tasks = {}
        for example in tqdm(self.dataset.dataset):
            code = example["code"] + "\n" + example["test_list"][0]
            pred_toks = model.generate(code, tokenizer, num_tokens, decode=False)

            # TODO: Come up with a good heuristic for merging these. One idea might be to just try all different number of tokens produced by the model and if any of them pass the assertion, then we can consider it a success. This is a bit of a hack, but it might be good enough and we just say that future work can be done to improve this. We can also reduce the amount of code we have to run by checking if each of the completions we make is valid code and only trying those.
            
            # Generate all valid completions by greedily merging every subset of tokens
            completions = merge_completions(example["code"], pred_toks, tokenizer)
            
            tasks[example["task_id"]] = {}
            # if there are no completions, then put failure for all asserts
            if len(completions) == 0:
                tasks[example["task_id"]]["easy_asserts"] = [0] * len(example["test_list"])
                tasks[example["task_id"]]["hard_asserts"] = [0] * len(example["challenge_test_list"])
                tasks[example["task_id"]]["easy_completed_code"] = None
                tasks[example["task_id"]]["hard_completed_code"] = None
                continue
            
            # loop through each completion and check if they pass the asserts
            all_ez_asserts, all_hard_asserts = [], []
            for completion in completions:
                ez_asserts = [assert_functionality(completion, assertion) for assertion in example["test_list"]]
                hard_asserts = [assert_functionality(completion, assertion) for assertion in example["challenge_test_list"]]

                all_ez_asserts.append(ez_asserts)
                all_hard_asserts.append(hard_asserts)
            
            # find index of maximum number of successful assertions from: https://stackoverflow.com/a/11530997/5768407
            max_ez_id = max(enumerate(all_ez_asserts),key=lambda x: sum(x[1]))[0]
            max_hard_id = max(enumerate(all_hard_asserts),key=lambda x: sum(x[1]))[0]

            # set the asserts as the code that passed the most asserts
            tasks[example["task_id"]]["easy_asserts"] = all_ez_asserts[max_ez_id]
            tasks[example["task_id"]]["hard_asserts"] = all_hard_asserts[max_hard_id]
            tasks[example["task_id"]]["easy_completed_code"] = completions[max_ez_id]
            tasks[example["task_id"]]["hard_completed_code"] = completions[max_hard_id]

        # flatten the list of lists
        easy_asserts = [item for sublist in tasks.values() for item in sublist["easy_asserts"]]
        hard_asserts = [item for sublist in tasks.values() for item in sublist["hard_asserts"]]

        # calculate the accuracy of each type of assert
        if len(easy_asserts) != 0:
            print(f"Accuracy on easy asserts: {sum(easy_asserts) / len(easy_asserts)}")
        if len(hard_asserts) != 0:
            print(f"Accuracy on hard asserts: {sum(hard_asserts) / len(hard_asserts)}")

        # count number of non-completed code
        num_non_completed_code = 0
        for task in tasks.values():
            if task["easy_completed_code"] is None:
                num_non_completed_code += 1
        
        print(f"Number of non-completed code: {num_non_completed_code}")

        return tasks

# %% ../nbs/03_eval.ipynb 10
def merge_completions(code, pred_tokens, tokenizer, mask_token="<mask>"):
    # Generate all valid completions by greedily merging every subset of tokens
    
    completions = []
    for i in range(len(pred_tokens)):
        pred = tokenizer.decode(pred_tokens[:i + 1], skip_special_tokens=True)
        completion = code.replace(mask_token, pred)
        # from: https://stackoverflow.com/a/35796569
        try:
            ast.parse(completion)
            completions.append(completion)
        except SyntaxError as e:
            continue
    
    return completions

# Code modified from: https://github.com/huggingface/transformers/blob/master/examples/research_projects/codeparrot/scripts/human_eval.py
def first_block(prompt, completion):
    """Split off first block of code by scanning for class, def etc. on newlines."""
    completion = completion.replace(prompt, "")
    completion = prompt + re.split("\nclass|\ndef|\n#|\n@|\nprint|\nif", completion)[0].rstrip()
    return completion


def complete_code(model, tokenizer, prompt, num_completions=32, num_samples=256):
    """Complete prompt with text generation pipeline and return num_completions."""
    code_gen_tokens = [
        model.generate(prompt, tokenizer, num_samples, decode=False)
        for _ in range(num_completions)
    ]
    all_completions = [merge_completions(prompt, tokens, tokenizer) for tokens in code_gen_tokens]
    print(all_completions[0][0])
    return [first_block(prompt, code_gen) for completions in all_completions for code_gen in completions]

# %% ../nbs/03_eval.ipynb 11
PY_LANGUAGE = Language(str(Path(completeformer.__path__[0])/"tree-sitter-languages.so"), "python")
parser = Parser()
parser.set_language(PY_LANGUAGE)

# get all the blocks from a given method sorted by length (ascending)
def get_block_offsets(method):
    tree = parser.parse(bytes(method, "utf8"))
    # From: https://github.com/github/CodeSearchNet/tree/master/function_parser
    def traverse(node, results) -> None:
        if node.type == "block":
            results.append(node)
            # return
        for n in node.children:
            traverse(n, results)
    blocks = []
    traverse(tree.root_node, blocks)

    def get_offset(node):
        lines = method.split("\n")
        def convert_to_offset(point, lines):
            row, column = point
            chars_in_rows = sum(map(len, lines[:row])) + row
            chars_in_columns = len(lines[row][:column])

            offset = chars_in_rows + chars_in_columns
            return offset
        start_offset = convert_to_offset(node.start_point, lines)
        end_offset = convert_to_offset(node.end_point, lines)

        return start_offset, end_offset
    
    offsets = [get_offset(b) for b in blocks]
    return offsets

# %% ../nbs/03_eval.ipynb 12
# Code modified from: https://github.com/huggingface/transformers/blob/master/examples/research_projects/codeparrot/scripts/human_eval.py
class HumanEvalEvaluator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.all_tasks = []
        self.prompts = {"sm": [], "md": [], "lg": []}
        # Load evaluation dataset and metric
        self.human_eval = load_dataset("openai_humaneval")["test"]
        self.human_eval = self.human_eval.map(self.construct_dataset)
        self.code_eval_metric = load_metric("code_eval")

        lens = []
        for _, solution, *_ in self.all_tasks:
            lens.append(len(tokenizer.tokenize(solution)))
        
        # bin lengths into small, medium, large
        lens = np.array(lens)
        bins = np.quantile(lens, [0.25, 0.5, 0.75])
        print("Bins:", bins)
        self.lens_bins = np.digitize(lens, bins)

        # add prompts depending on their length
        for i, (prompt, solution, test, entry_point) in enumerate(self.all_tasks):
            if self.lens_bins[i] == 1:
                self.prompts["sm"].append((prompt, solution, test, entry_point))
            elif self.lens_bins[i] == 2:
                self.prompts["md"].append((prompt, solution, test, entry_point))
            else:
                self.prompts["lg"].append((prompt, solution, test, entry_point))

    
    def construct_dataset(self, example):
        method = example["prompt"] + example["canonical_solution"]
        offsets = get_block_offsets(method)
        example["masked_prompts"], example["solutions"] = [], []
        for offset in offsets:
            # From: https://stackoverflow.com/a/66424988/5768407
            masked_prompt = list(method)
            masked_prompt[offset[0]:offset[1]] = [self.tokenizer.mask_token]
            masked_prompt = "".join(masked_prompt)
            solution = method[offset[0]:offset[1]]
            example["masked_prompts"].append(masked_prompt)
            example["solutions"].append(solution)

            self.all_tasks.append((masked_prompt, solution, example["test"], example["entry_point"]))

        return example
    
    def evaluate(self, model, output_dir, n_samples=100):
        os.environ["HF_ALLOW_CODE_EVAL"] = "1" # Allow executing code that the model generates
        for key, prompts in self.prompts.items():
            print(f"Evaluating {key} prompts...")
            # Generate completion for each prompt
            generations, references = [], []
            for prompt, solution, test, entry_point in prompts:
                generations.append(complete_code(model, tokenizer, prompt, num_completions=n_samples))
                references.append("\n" + test + "\n" + entry_point)
            
            # Evaluate completions with "code_eval" metric
            pass_at_k, _ = self.code_eval_metric.compute(
                references=references, predictions=generations
            )
            print(f"{key} pass at k: {pass_at_k}")
            with open(os.path.join(output_dir, f"{key}_pass_at_k.json"), "w") as f:
                json.dump(pass_at_k, f)
    
    # def evaluate(self, model, output_file, n_samples=32):
    #     # Generate completion for each task
    #     generations, references = [], []
    #     for task in tqdm(self.human_eval):
    #         generations.append(complete_code(model, tokenizer, task["masked_prompts"], num_completions=n_samples))
    #         test_func = task["test"]
    #         entry_point = f"check({task['entry_point']})"
    #         references.append("\n" + test_func + "\n" + entry_point)
        
    #     # Evaluate completions with "code_eval" metric
    #     pass_at_k, _ = self.code_eval_metric.compute(
    #         references=references, predictions=generations
    #     )
    #     print(f"Results: {pass_at_k}")

    #     # Save results to json file
    #     with open(output_file, "w") as fp:
    #         json.dump(pass_at_k, fp)
        
    #     return results

    # def evaluate(self, model, tokenizer, output_file, mask_length=None, n_samples=32, n_tasks=None):
    #     """
    #     """
    #     os.environ["HF_ALLOW_CODE_EVAL"] = "1" # Allow executing code that the model generates

    #     if n_tasks is None:
    #         n_tasks = len(self.human_eval)
    #     generations, references = [], []
    #     for task in tqdm(range(n_tasks)):
    #         task_generations = []
    #         prompt = self.human_eval[task]["prompt"].strip()
            
    #         task_generations.extend(
    #             complete_code(model, tokenizer, prompt + tokenizer.mask_token, num_completions=n_samples)
    #         )
    #         generations.append([prompt + gen for gen in task_generations])
    #         test_func = self.human_eval[task]["test"]
    #         entry_point = f"check({self.human_eval[task]['entry_point']})"
    #         references.append("\n" + test_func + "\n" + entry_point)


    #     # Evaluate completions with "code_eval" metric
    #     pass_at_k, _ = self.code_eval_metric.compute(
    #         references=references, predictions=generations
    #     )
    #     print(f"Results: {pass_at_k}")

    #     # Save results to json file
    #     with open(output_file, "w") as fp:
    #         json.dump(pass_at_k, fp)
