{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: train.html\n",
    "title: Train\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-10 21:49:16.743509: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-10 21:49:17.177108: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-10 21:49:17.177150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64:\n",
      "2023-03-10 21:49:17.177154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from codecarbon import EmissionsTracker\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset completeformer (/work/.cache/huggingface/datasets/semeru___completeformer/short/1.1.0/73b388644fdc749387b0ed1f17dad849561b5ca75db0faa9bfdecd5e8d6d1d91)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86e7b3984d84ee5acd5bb63369ec6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd2e3852b9042c581b2c778ac7c02d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3faaa9fe67a43a0a88ebd1bf3a65131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c74f5561e146dc97cfbff2c75d52b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9609335111544437a5438d81a6becce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/71 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6e632be2ba45148d64429302c2740d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32fd56aeaed414d8254776d7760b717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45410de3faa049e7970e9888db927a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f91ca2d905406b9b529e3f76fd2476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c9ff49d60243b49dea53f34de417a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4e1a18ef6e480797c50cafb08910c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a46a4f1fce943ee9d952c67f4dadab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffc7d5a827142a0813acb58a7824da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| include: false\n",
    "from completeformer.data import CompleteformerDataset\n",
    "from completeformer.models import Completeformer\n",
    "from pathlib import Path\n",
    "\n",
    "config = {\n",
    "    \"position_type\": \"sinusoidal\",\n",
    "    \"dim\": 512,\n",
    "    \"enc_max_len\": 512,\n",
    "    \"enc_layers\": 6,\n",
    "    \"enc_heads\": 8,\n",
    "    \"dec_max_len\": 256,\n",
    "    \"dec_layers\": 6,\n",
    "    \"dec_heads\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_warmup_steps\": 20,\n",
    "}\n",
    "\n",
    "ENC_MAX_LEN = 1024\n",
    "DEC_MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "short_ds = CompleteformerDataset(\n",
    "    length=\"short\",\n",
    "    tokenizer_name=\"semeru/completeformer_tokenizer\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    enc_max_len=ENC_MAX_LEN,\n",
    "    dec_max_len=DEC_MAX_LEN,\n",
    "    num_workers=4,\n",
    ")\n",
    "short_ds.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def train(\n",
    "    model,\n",
    "    data_module,\n",
    "    num_epochs,\n",
    "    output_dir,\n",
    "    name=None,\n",
    "    limit_train_batches=1.0,\n",
    "    limit_val_batches=1.0,\n",
    "    accumulate_grad_batches=1,\n",
    "    val_check_interval=20_000,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Train a model with a given training data loader, validation data loader,\n",
    "    optimizer, scheduler, loss function, metrics, and callbacks.\n",
    "\n",
    "    Args:\n",
    "        model (LightningModule): The model to train.\n",
    "        train_dataloader (DataLoader): The training data loader.\n",
    "        val_dataloader (DataLoader): The validation data loader.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        output_dir (str): The directory to save the model to.\n",
    "        name (str): The name of the model.\n",
    "    Returns:\n",
    "        best_model_path (str): The path to the best model's checkpoint.\n",
    "    \"\"\"\n",
    "    pl.seed_everything(115, workers=True)\n",
    "    wandb_logger = WandbLogger(project=\"Completeformer\", name=name)\n",
    "    # saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
    "    checkpoint_path = output_dir / \"checkpoints\"\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=str(checkpoint_path),\n",
    "        filename=\"completeformer-{epoch:02d}-{val_loss:.2f}\",\n",
    "        save_top_k=5,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        log_every_n_steps=1,\n",
    "        logger=wandb_logger,\n",
    "        default_root_dir=str(checkpoint_path),\n",
    "        gpus=torch.cuda.device_count(),\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        max_epochs=num_epochs,\n",
    "        limit_train_batches=limit_train_batches,\n",
    "        limit_val_batches=limit_val_batches,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            TQDMProgressBar(refresh_rate=1),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    # save the last model\n",
    "    trainer.save_checkpoint(str(checkpoint_path / \"final_checkpoint.ckpt\"))\n",
    "\n",
    "    # save the best model to wandb\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    if best_model_path is not None:\n",
    "        wandb.save(best_model_path)\n",
    "\n",
    "    return model, checkpoint_callback.best_model_path, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nathan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nathan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/nathan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Global seed set to 115\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatedog\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230310_214944-hdvoz86y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/natedog/Completeformer/runs/hdvoz86y\" target=\"_blank\">test</a></strong> to <a href=\"https://wandb.ai/natedog/Completeformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nathan/miniconda3/envs/completeformer/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/nathan/miniconda3/envs/completeformer/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /tmp/completeformer/models/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/nathan/miniconda3/envs/completeformer/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | XTransformer | 111 M \n",
      "---------------------------------------\n",
      "111 M     Trainable params\n",
      "0         Non-trainable params\n",
      "111 M     Total params\n",
      "223.363   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f486714ff7d641b2a8dc9942dd6feb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bcd6093135465f8b878fae2637f626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5817d2e389247d7ac892c7190d0f0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    }
   ],
   "source": [
    "# write some unit tests to test the above function\n",
    "num_epochs = 1\n",
    "output_dir = Path(\"/tmp/completeformer/models\")\n",
    "\n",
    "model = Completeformer(short_ds.tokenizer, length=\"short\", max_epochs=num_epochs, **config)\n",
    "model, best_model_path, trainer = train(\n",
    "    model,\n",
    "    short_ds,\n",
    "    num_epochs=num_epochs,\n",
    "    output_dir=output_dir,\n",
    "    name=\"test\",\n",
    "    limit_train_batches=0.1,\n",
    "    limit_val_batches=0.1,\n",
    "    val_check_interval=124,\n",
    ")\n",
    "\n",
    "# evaluate that the best model path exists\n",
    "assert Path(best_model_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822e5d15a2c14b94b0762b03905ac08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        bleu_short         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.09326142817735672    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        chrf_short         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    28.752025604248047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     exact_match_short     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     leven_dist_short      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8018017197124175     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       meteor_short        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.46674807470162394    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        rouge_short        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.290683514636622     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         tst_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.730475902557373     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       bleu_short        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.09326142817735672   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       chrf_short        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   28.752025604248047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    exact_match_short    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    leven_dist_short     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8018017197124175    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      meteor_short       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.46674807470162394   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       rouge_short       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.290683514636622    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        tst_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.730475902557373    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "short_results = trainer.test(model=model, dataloaders=short_ds.test_dataloader())[0]\n",
    "\n",
    "{'tst_loss': 3.730475902557373,\n",
    " 'bleu_short': 0.09326142817735672,\n",
    " 'chrf_short': 28.752025604248047,\n",
    " 'exact_match_short': 0.0,\n",
    " 'leven_dist_short': 0.8018017197124175,\n",
    " 'meteor_short': 0.46674807470162394,\n",
    " 'rouge_short': 0.290683514636622}\n",
    "\n",
    "assert short_results.keys() == {\n",
    "    \"tst_loss\",\n",
    "    \"bleu_short\",\n",
    "    \"chrf_short\",\n",
    "    \"exact_match_short\",\n",
    "    \"leven_dist_short\",\n",
    "    \"meteor_short\",\n",
    "    \"rouge_short\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
